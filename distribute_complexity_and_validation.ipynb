{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ml1-arm64",
      "language": "python",
      "name": "ml1-arm64"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "distribute_complexity_and_validation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlizaShamsi/LearningAModel/blob/main/distribute_complexity_and_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide": true,
        "id": "JsLDHjHFTVQu"
      },
      "source": [
        "# Validation and Regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide": true,
        "id": "pR1cIqCPTVRJ"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide": true,
        "id": "Jc0dky0TTVRM"
      },
      "source": [
        "def make_simple_plot():\n",
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$y$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([-2,2])\n",
        "    axes[1].set_ylim([-2,2])\n",
        "    plt.tight_layout();\n",
        "    return axes\n",
        "def make_plot():\n",
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$p_R$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([0,1])\n",
        "    axes[1].set_ylim([0,1])\n",
        "    axes[0].set_xlim([0,1])\n",
        "    axes[1].set_xlim([0,1])\n",
        "    plt.tight_layout();\n",
        "    return axes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21iaAz-NTVRO"
      },
      "source": [
        "## PART 1: Reading in and sampling from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "P7duQoApTVRP",
        "outputId": "5b6f9945-cae2-4909-838d-1e84400470db"
      },
      "source": [
        "!git init; git pull https://github.com/AlizaShamsi/LearningAModel.git\n",
        "df=pd.read_csv(\"data/noisypopulation.csv\")\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/AlizaShamsi/LearningAModel\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Updating 997ba99..6e871af\n",
            "Fast-forward\n",
            " distribute_complexity_and_validation.ipynb | 409 \u001b[32m+++++++++++++++++++\u001b[m\u001b[31m----------\u001b[m\n",
            " 1 file changed, 272 insertions(+), 137 deletions(-)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.047790</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.011307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.051199</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.054799</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.007237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058596</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.000056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.062597</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          f     x         y\n",
              "0  0.047790  0.00  0.011307\n",
              "1  0.051199  0.01  0.010000\n",
              "2  0.054799  0.02  0.007237\n",
              "3  0.058596  0.03  0.000056\n",
              "4  0.062597  0.04  0.010000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVejxwkTVRQ"
      },
      "source": [
        "x=df.f.values\n",
        "f=df.x.values\n",
        "y=df.y.values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NfZPQPwTVRR",
        "outputId": "d9e83ca0-c3a6-48a5-b195-0245f12d8e39"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XePk5DkTTVRS"
      },
      "source": [
        "From 200 points on this curve, we'll make a random choice of 60 points. We do it by choosing the indexes randomly, and then using these indexes as a way of grtting the appropriate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJZ-b_KeTVRS",
        "outputId": "00550ed8-8364-4681-8078-7bad60580e1b"
      },
      "source": [
        "indexes=np.sort(np.random.choice(x.shape[0], size=60, replace=False))\n",
        "indexes"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   3,  12,  13,  16,  20,  24,  25,  30,  32,  34,  36,  37,\n",
              "        45,  46,  51,  53,  54,  55,  56,  57,  63,  64,  70,  71,  75,\n",
              "        76,  85,  86,  87,  89,  90,  91,  93,  98, 103, 106, 107, 108,\n",
              "       110, 134, 135, 141, 146, 147, 156, 157, 164, 168, 169, 172, 173,\n",
              "       174, 178, 183, 191, 192, 194, 195, 198])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXPW4HYlTVRT"
      },
      "source": [
        "samplex = x[indexes]\n",
        "samplef = f[indexes]\n",
        "sampley = y[indexes]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "figure_type": "m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "jvEuFYiyTVRU",
        "outputId": "ff961c8b-ca9e-4b35-fae1-aaa921e40aa8"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x,f, 'k-', alpha=0.6, label=\"f\");\n",
        "plt.plot(x[indexes], y[indexes], 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\");\n",
        "plt.plot(x, y, '.', alpha=0.8, label=\"population y\");\n",
        "plt.xlabel('x');\n",
        "plt.ylabel('y')\n",
        "plt.legend(loc=4);"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "09Xxnu-eTVRV",
        "outputId": "bc09ca93-34e6-49a9-aecf-8c28ee70f020"
      },
      "source": [
        "sample_df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))\n",
        "sample_df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>f</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.047790</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058596</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.000056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.102637</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.021021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.108726</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.171233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.128537</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.083420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.158655</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.157406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.193062</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.117586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.202328</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.241355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.252493</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.259972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.274253</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.253819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.296901</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.308538</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.349239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.320369</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.304457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.417548</td>\n",
              "      <td>0.437549</td>\n",
              "      <td>0.492085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.420740</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.408499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.486704</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>0.530707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.493411</td>\n",
              "      <td>0.495045</td>\n",
              "      <td>0.510697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.542010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.513296</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.540886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.526576</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.470946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.528242</td>\n",
              "      <td>0.521256</td>\n",
              "      <td>0.521578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.553035</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.619842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.566184</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.481262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.606590</td>\n",
              "      <td>0.581133</td>\n",
              "      <td>0.699642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.609425</td>\n",
              "      <td>0.583346</td>\n",
              "      <td>0.661606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.624457</td>\n",
              "      <td>0.595162</td>\n",
              "      <td>0.633472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.626062</td>\n",
              "      <td>0.596433</td>\n",
              "      <td>0.692373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.655422</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.696182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.655747</td>\n",
              "      <td>0.620265</td>\n",
              "      <td>0.778489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.658746</td>\n",
              "      <td>0.622713</td>\n",
              "      <td>0.676026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.668448</td>\n",
              "      <td>0.630689</td>\n",
              "      <td>0.659203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.668477</td>\n",
              "      <td>0.630714</td>\n",
              "      <td>0.655974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.669625</td>\n",
              "      <td>0.631663</td>\n",
              "      <td>0.649977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.676189</td>\n",
              "      <td>0.637121</td>\n",
              "      <td>0.606263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.684512</td>\n",
              "      <td>0.644106</td>\n",
              "      <td>0.624411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.701778</td>\n",
              "      <td>0.658856</td>\n",
              "      <td>0.780488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.711363</td>\n",
              "      <td>0.667211</td>\n",
              "      <td>0.540834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.713591</td>\n",
              "      <td>0.669172</td>\n",
              "      <td>0.790602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.714352</td>\n",
              "      <td>0.669843</td>\n",
              "      <td>0.787620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.716490</td>\n",
              "      <td>0.671733</td>\n",
              "      <td>0.635691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.772698</td>\n",
              "      <td>0.724329</td>\n",
              "      <td>0.925265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.776003</td>\n",
              "      <td>0.727629</td>\n",
              "      <td>0.677394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.789744</td>\n",
              "      <td>0.741660</td>\n",
              "      <td>0.831687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.797672</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.787785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.799157</td>\n",
              "      <td>0.751585</td>\n",
              "      <td>0.706604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.815940</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.831663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.820176</td>\n",
              "      <td>0.774811</td>\n",
              "      <td>0.704145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.841345</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.814451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.851742</td>\n",
              "      <td>0.813180</td>\n",
              "      <td>0.848953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.853323</td>\n",
              "      <td>0.815237</td>\n",
              "      <td>0.798033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.859389</td>\n",
              "      <td>0.823273</td>\n",
              "      <td>0.777734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.863248</td>\n",
              "      <td>0.828509</td>\n",
              "      <td>0.766389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.864334</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>0.892565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.866761</td>\n",
              "      <td>0.833363</td>\n",
              "      <td>0.904599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.884930</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.877267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.916956</td>\n",
              "      <td>0.915466</td>\n",
              "      <td>0.912887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.919243</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.908517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.928767</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.936134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.933193</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.889002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.945201</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.920384</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           x         f         y\n",
              "0   0.047790  0.000000  0.011307\n",
              "1   0.058596  0.030000  0.000056\n",
              "2   0.102637  0.120000  0.021021\n",
              "3   0.108726  0.130000  0.171233\n",
              "4   0.128537  0.160000  0.083420\n",
              "5   0.158655  0.200000  0.157406\n",
              "6   0.193062  0.240000  0.117586\n",
              "7   0.202328  0.250000  0.241355\n",
              "8   0.252493  0.300000  0.259972\n",
              "9   0.274253  0.320000  0.253819\n",
              "10  0.296901  0.340000  0.236000\n",
              "11  0.308538  0.350000  0.349239\n",
              "12  0.320369  0.360000  0.304457\n",
              "13  0.417548  0.437549  0.492085\n",
              "14  0.420740  0.440000  0.408499\n",
              "15  0.486704  0.490000  0.530707\n",
              "16  0.493411  0.495045  0.510697\n",
              "17  0.500000  0.500000  0.542010\n",
              "18  0.513296  0.510000  0.540886\n",
              "19  0.526576  0.520000  0.470946\n",
              "20  0.528242  0.521256  0.521578\n",
              "21  0.553035  0.540000  0.619842\n",
              "22  0.566184  0.550000  0.481262\n",
              "23  0.606590  0.581133  0.699642\n",
              "24  0.609425  0.583346  0.661606\n",
              "25  0.624457  0.595162  0.633472\n",
              "26  0.626062  0.596433  0.692373\n",
              "27  0.655422  0.620000  0.696182\n",
              "28  0.655747  0.620265  0.778489\n",
              "29  0.658746  0.622713  0.676026\n",
              "30  0.668448  0.630689  0.659203\n",
              "31  0.668477  0.630714  0.655974\n",
              "32  0.669625  0.631663  0.649977\n",
              "33  0.676189  0.637121  0.606263\n",
              "34  0.684512  0.644106  0.624411\n",
              "35  0.701778  0.658856  0.780488\n",
              "36  0.711363  0.667211  0.540834\n",
              "37  0.713591  0.669172  0.790602\n",
              "38  0.714352  0.669843  0.787620\n",
              "39  0.716490  0.671733  0.635691\n",
              "40  0.772698  0.724329  0.925265\n",
              "41  0.776003  0.727629  0.677394\n",
              "42  0.789744  0.741660  0.831687\n",
              "43  0.797672  0.750000  0.787785\n",
              "44  0.799157  0.751585  0.706604\n",
              "45  0.815940  0.770000  0.831663\n",
              "46  0.820176  0.774811  0.704145\n",
              "47  0.841345  0.800000  0.814451\n",
              "48  0.851742  0.813180  0.848953\n",
              "49  0.853323  0.815237  0.798033\n",
              "50  0.859389  0.823273  0.777734\n",
              "51  0.863248  0.828509  0.766389\n",
              "52  0.864334  0.830000  0.892565\n",
              "53  0.866761  0.833363  0.904599\n",
              "54  0.884930  0.860000  0.877267\n",
              "55  0.916956  0.915466  0.912887\n",
              "56  0.919243  0.920000  0.908517\n",
              "57  0.928767  0.940000  0.936134\n",
              "58  0.933193  0.950000  0.889002\n",
              "59  0.945201  0.980000  0.920384"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VemWnMXTVRW"
      },
      "source": [
        "## Part 2: Fit on training set and predict on test set\n",
        "\n",
        "We will do the split of testing and training for you in order to illustrate how this can be done.\n",
        "\n",
        "### Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA7lFsB1TVRX",
        "outputId": "f23c65c0-d5a4-4563-9375-64a669f118ae"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "datasize=sample_df.shape[0]\n",
        "print(datasize)\n",
        "#split dataset using the index, as we have x,f, and y that we want to split.\n",
        "itrain,itest = train_test_split(np.arange(60),train_size=0.8)\n",
        "print(itrain.shape)\n",
        "xtrain= sample_df.x[itrain].values\n",
        "ftrain = sample_df.f[itrain].values\n",
        "ytrain = sample_df.y[itrain].values\n",
        "xtest= sample_df.x[itest].values\n",
        "ftest = sample_df.f[itest].values\n",
        "ytest = sample_df.y[itest].values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "(48,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcz8gZe3TVRX",
        "outputId": "ec58ffe1-ef96-4ed9-dd24-91a3e74e5aa3"
      },
      "source": [
        "sample_df.x[itrain].values"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.85938891, 0.91695631, 0.86324832, 0.62606244, 0.41754779,\n",
              "       0.6845117 , 0.60942486, 0.60658999, 0.71359101, 0.6684476 ,\n",
              "       0.19306234, 0.85332261, 0.55303512, 0.6761895 , 0.82017586,\n",
              "       0.66847716, 0.88493033, 0.29690143, 0.77600333, 0.12853715,\n",
              "       0.91924334, 0.65874566, 0.71136277, 0.71648951, 0.42074029,\n",
              "       0.52657646, 0.25249254, 0.27425312, 0.30853754, 0.05859631,\n",
              "       0.84134475, 0.85174194, 0.94520071, 0.20232838, 0.81593987,\n",
              "       0.86676103, 0.65542174, 0.10872571, 0.79767162, 0.51329561,\n",
              "       0.65574675, 0.66962481, 0.9331928 , 0.56618383, 0.71435176,\n",
              "       0.62445669, 0.48670439, 0.77269819])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFlTrcMTVRY"
      },
      "source": [
        "We'll need to create polynomial features, ie add 1, x, x^2 and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0XkmImqTVRY"
      },
      "source": [
        "### The `scikit-learn` interface\n",
        "\n",
        "Scikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in python by the incantation `import sklearn`.\n",
        "\n",
        "The library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the [scikit-learn API paper](http://arxiv.org/pdf/1309.0238v1.pdf) [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).] says:\n",
        "\n",
        ">All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: **an estimator interface for building and tting models, a predictor interface for making predictions and a transformer interface for converting data**. The estimator interface is at the core of the library. It denes instantiation mechanisms of objects and exposes a `fit` method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classication, regression or clustering) are oered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n",
        "\n",
        "We'll use the \"estimator\" interface here, specifically the estimator `PolynomialFeatures`. The API paper again:\n",
        "\n",
        ">Since it is common to modify or lter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which denes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n",
        "\n",
        "To start with we have one **feature** `x` to predict `y`, what we will do is the transformation:\n",
        "\n",
        "$$ x \\rightarrow 1, x, x^2, x^3, ..., x^d $$\n",
        "\n",
        "for some power $d$. Our job then is to **fit** for the coefficients of these features in the polynomial\n",
        "\n",
        "$$ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. $$\n",
        "\n",
        "In other words, we have transformed a function of one feature, into a (rather simple) **linear** function of many features. To do this we first construct the estimator as `PolynomialFeatures(d)`, and then transform these features into a d-dimensional space using the method `fit_transform`.\n",
        "\n",
        "![fit_transform](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/sklearntrans.jpg?raw=1)\n",
        "\n",
        "Here is an example. The reason for using `[[1],[2],[3]]` as opposed to `[1,2,3]` is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size `[n_samples, n_features]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOiDX6dGTVRb",
        "outputId": "3bb60728-55e3-4d71-be23-953f49250b10"
      },
      "source": [
        "np.array([1,2,3]).reshape(-1,1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2],\n",
              "       [3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S49ok6v_TVRc"
      },
      "source": [
        "To transform `[1,2,3]` into [[1],[2],[3]] we need to do a reshape.\n",
        "\n",
        "![reshape](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/reshape.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RvRQMGMTVRc",
        "outputId": "455cb9bc-52e7-4e82-eedd-218c67063e24"
      },
      "source": [
        "xtest.reshape(-1,1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.52824238],\n",
              "       [0.15865525],\n",
              "       [0.5       ],\n",
              "       [0.04779035],\n",
              "       [0.49341117],\n",
              "       [0.10263725],\n",
              "       [0.86433394],\n",
              "       [0.70177752],\n",
              "       [0.32036919],\n",
              "       [0.79915732],\n",
              "       [0.78974405],\n",
              "       [0.92876662]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ez-Wk0FTVRd"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEddC1Knbiro"
      },
      "source": [
        "#PolynomialFeatures(3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "SaB5IMW3TVRd",
        "outputId": "55f466ad-8e9c-429f-cbdd-2d3920b2a91e"
      },
      "source": [
        "PolynomialFeatures(3).fit_transform(xtest)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-63babab40c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \"\"\"\n\u001b[0;32m-> 1508\u001b[0;31m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         combinations = self._combinations(n_features, self.degree,\n\u001b[1;32m   1510\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.52824238 0.15865525 0.5        0.04779035 0.49341117 0.10263725\n 0.86433394 0.70177752 0.32036919 0.79915732 0.78974405 0.92876662].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaVVZsE1TVRe",
        "outputId": "5c59d6d5-7266-4e75-d77f-a7acb4f8e731"
      },
      "source": [
        "PolynomialFeatures(3).fit_transform(xtest.reshape(-1,1))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e+00, 5.28242384e-01, 2.79040016e-01, 1.47400763e-01],\n",
              "       [1.00000000e+00, 1.58655254e-01, 2.51714896e-02, 3.99358907e-03],\n",
              "       [1.00000000e+00, 5.00000000e-01, 2.50000000e-01, 1.25000000e-01],\n",
              "       [1.00000000e+00, 4.77903523e-02, 2.28391777e-03, 1.09149235e-04],\n",
              "       [1.00000000e+00, 4.93411169e-01, 2.43454582e-01, 1.20123210e-01],\n",
              "       [1.00000000e+00, 1.02637252e-01, 1.05344055e-02, 1.08122243e-03],\n",
              "       [1.00000000e+00, 8.64333939e-01, 7.47073158e-01, 6.45720686e-01],\n",
              "       [1.00000000e+00, 7.01777520e-01, 4.92491688e-01, 3.45619596e-01],\n",
              "       [1.00000000e+00, 3.20369191e-01, 1.02636418e-01, 3.28815463e-02],\n",
              "       [1.00000000e+00, 7.99157321e-01, 6.38652424e-01, 5.10383760e-01],\n",
              "       [1.00000000e+00, 7.89744050e-01, 6.23695664e-01, 4.92559940e-01],\n",
              "       [1.00000000e+00, 9.28766623e-01, 8.62607439e-01, 8.01160998e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MHZpiDoTVRe"
      },
      "source": [
        "### Creating Polynomial features\n",
        "\n",
        "We'll write a function to encapsulate what we learnt about creating the polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbe5CKu-TVRf"
      },
      "source": [
        "def make_features(train_set, test_set, degrees):\n",
        "    train_dict = {}\n",
        "    test_dict = {}\n",
        "    for d in degrees:\n",
        "        #traintestdict={}\n",
        "        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
        "        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
        "    return train_dict, test_dict"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhN9eD-8TVRf"
      },
      "source": [
        "### Doing the fit\n",
        "\n",
        "We first create our features, and some arrays to store the errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAAQjlHaTVRg"
      },
      "source": [
        "degrees=range(21)\n",
        "train_dict, test_dict = make_features(xtrain, xtest, degrees)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_p1zTsdTVRg",
        "outputId": "d5022c73-5d04-4542-d534-12e975ebeb35"
      },
      "source": [
        "error_train=np.empty(len(degrees))\n",
        "error_test=np.empty(len(degrees))\n",
        "error_test.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMsHWUNOTVRh"
      },
      "source": [
        "What is the fitting process? We first loop over all the **hypothesis set**s that we wish to consider: in our case this is a loop over the complexity parameter $d$, the degree of the polynomials we will try and fit. That is we start with ${\\cal H_0}$, the set of all 0th order polynomials, then do ${\\cal H_1}$, then ${\\cal H_2}$, and so on... We use the notation ${\\cal H}$ to indicate a hypothesis set. Then for each degree $d$, we obtain a best fit model. We then \"test\" this model by predicting on the test chunk, obtaining the test set error for the best-fit polynomial coefficients and for degree $d$. We move on to the next degree $d$ and repeat the process, just like before. We compare all the test set errors, and pick the degree $d_*$ and the model in ${\\cal H_{d_*}}$ which minimizes this test set error.\n",
        "\n",
        ">**YOUR TURN HERE**: For each degree d, train on the training set and predict on the test set. Store the training MSE in `error_train` and test MSE in `error_test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHDWbZSkTVRi"
      },
      "source": [
        "#for each degree, we now fit on the training set and predict on the test set\n",
        "#we accumulate the MSE on both sets in error_train and error_test\n",
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n",
        "    Xtrain = train_dict[d]\n",
        "    Xtest = test_dict[d]\n",
        "    #set up model\n",
        "    est = LinearRegression()\n",
        "    #fit\n",
        "    est.fit(Xtrain, ytrain)\n",
        "    #predict\n",
        "    #your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmsKm5oVTVRi"
      },
      "source": [
        "We can find the best degree thus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZT4KyKbTVRj"
      },
      "source": [
        "bestd = np.argmin(error_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_DzlLK3TVRj"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n",
        "plt.plot(degrees, error_test, marker='o', label='test')\n",
        "plt.axvline(bestd, 0,0.5, color='r', label=\"min test error at d=%d\"%bestd, alpha=0.3)\n",
        "plt.ylabel('mean squared error')\n",
        "plt.xlabel('degree')\n",
        "plt.legend(loc='upper left')\n",
        "plt.yscale(\"log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dzb0_HSTVRk"
      },
      "source": [
        "![m:caption](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/complexity-error-plot.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGvMNphuTVRm"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MwvyJ3TVRn"
      },
      "source": [
        "What we have done in picking a given $d$ as the best hypothesis is that we have used the test set as a training set. \n",
        "If we choose the best $d$ based on minimizing the test set error, we have then \"fit for\" hyperparameter $d$ on the test set. \n",
        "\n",
        "In this case, the test-set error will underestimate the true out of sample error. Furthermore, we have **contaminated the test set** by fitting for $d$ on it; it is no longer a true test set.\n",
        "\n",
        "Thus, we introduce a new **validation set** on which the complexity parameter $d$ is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n",
        "\n",
        "![m:caption](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/train-validate-test.png?raw=1)\n",
        "\n",
        "We have split the old training set into a **new smaller training set** and a **validation set**, holding the old test aside for FINAL testing AFTER we have \"fit\" for complexity $d$. Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample risk $\\cal{E}_{out}$ (also denoted as risk $R_{out}$) through the test risk $\\cal{E}_{test}$ ($R_{test}$).\n",
        "\n",
        "![m:caption](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/train-validate-test-cont.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78xdFDSVTVRo"
      },
      "source": [
        "The validation process is illustrated in these two figures. We first loop over the complexity parameter $d$, the degree of the polynomials we will try and fit. Then for each degree $d$, we obtain a best fit model $g^-_d$ where the \"minus\" superscript indicates that we fit our model on the new training set which is obtained by removing (\"minusing\") a validation chunk (often the same size as the test chunk) from the old training set. We then \"test\" this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree $d$. We move on to the next degree $d$ and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree $d_*$ which minimizes this validation set error.\n",
        "\n",
        "![caption](https://github.com/AlizaShamsi/LearningAModel/blob/main/images/train-validate-test3.png?raw=1)\n",
        "\n",
        "Having picked the hyperparameter $d_*$, we retrain using the hypothesis set $\\cal{H}_{*}$ on the entire old training-set to find the parameters of the polynomial of order $d_*$ and the corresponding best fit hypothesis $g_*$. Note that we left the minus off the $g$ to indicate that it was trained on the entire old traing set. We now compute the test error on the test set as an estimate of the test risk $\\cal{E}_{test}$.\n",
        "\n",
        "Thus the **validation** set if the set on which the hyperparameter is fit. This method of splitting the data $\\cal{D}$ is called the **train-validate-test** split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4Bk6m2TVRq"
      },
      "source": [
        "### Fit on training and predict on validation\n",
        "\n",
        "\n",
        "We carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "1k4LP0UDTVRq"
      },
      "source": [
        "#we split the training set down further\n",
        "intrain,invalid = train_test_split(itrain,train_size=36, test_size=12)\n",
        "# why not just use xtrain, xtest here? its the indices we need. How could you use xtrain and xtest?\n",
        "xntrain= sample_df.x[intrain].values\n",
        "fntrain = sample_df.f[intrain].values\n",
        "yntrain = sample_df.y[intrain].values\n",
        "xnvalid= sample_df.x[invalid].values\n",
        "fnvalid = sample_df.f[invalid].values\n",
        "ynvalid = sample_df.y[invalid].values\n",
        "\n",
        "degrees=range(21)\n",
        "train_dict, valid_dict = make_features(xntrain, xnvalid, degrees)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrPTPnH9TVRr"
      },
      "source": [
        "\n",
        ">YOUR TURN HERE: Train on the smaller training set. Fit for d on the validation set.  Store the respective MSEs in `error_train` and `error_valid`. Then retrain on the entire training set using this d. Label the test set MSE with the variable `err`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "rIa3ZNnnTVRs"
      },
      "source": [
        "error_train=np.empty(len(degrees))\n",
        "error_valid=np.empty(len(degrees))\n",
        "#for each degree, we now fit on the smaller training set and predict on the validation set\n",
        "#we accumulate the MSE on both sets in error_train and error_valid\n",
        "#we then find the degree of polynomial that minimizes the MSE on the validation set.\n",
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unExZh9hTVRs"
      },
      "source": [
        "#calculate the degree at which validation error is minimized\n",
        "mindeg = np.argmin(error_valid)\n",
        "mindeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "4CkvxzDITVRt"
      },
      "source": [
        "#fit on WHOLE training set now. \n",
        "##you will need to remake polynomial features on the whole training set\n",
        "#Put MSE on the test set in variable err.\n",
        "#your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LirXT5fNTVRu"
      },
      "source": [
        "We plot the training error and validation error against the degree of the polynomial, and show the test set error at the $d$ which minimizes the validation set error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnQIkoGNTVRu"
      },
      "source": [
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n",
        "plt.plot(degrees, error_valid, marker='o', label='validation')\n",
        "plt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n",
        "plt.ylabel('mean squared error')\n",
        "plt.xlabel('degree')\n",
        "plt.legend(loc='upper left')\n",
        "plt.yscale(\"log\")\n",
        "print(mindeg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqOsKFRjTVRu"
      },
      "source": [
        "> YOUR TURN HERE: Run the set of cells for the validation process again and again. What do you see? The validation error minimizing polynomial degree might change! What happened?\n"
      ]
    }
  ]
}